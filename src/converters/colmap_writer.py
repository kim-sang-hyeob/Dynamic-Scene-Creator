"""
COLMAP sparse format generation (cameras.txt, images.txt, points3D.txt).

Generates SfM-compatible files from Unity camera tracking data,
allowing 4DGS to bypass COLMAP entirely.
"""

import os
import math
import numpy as np
import cv2

from .coordinate import (
    normalize_position,
    get_map_transform,
    quat_to_mat,
    mat_to_quat,
    build_map_rotation_matrix,
)


def write_colmap_text(frames, output_dir, img_dir=None, map_transform=None, use_midas=True):
    """
    Writes cameras.txt, images.txt, and points3D.txt in COLMAP format.
    Includes proper Unity (LHS) to COLMAP (RHS) conversion with map transform normalization.

    For images with alpha channel, initial points are generated by back-projecting
    foreground pixels to 3D, ensuring Gaussians start where the object is.

    Args:
        frames: List of frame data with camPos, camRot
        output_dir: Directory to write COLMAP files
        img_dir: Directory containing images (for dimensions and alpha detection)
        map_transform: Unity map transform for coordinate conversion
        use_midas: If True, use MiDaS for depth-based point initialization
    """
    if map_transform is None:
        map_transform = get_map_transform()

    # 1. cameras.txt - Get actual image dimensions
    width, height = 1280, 720  # Default fallback
    has_alpha = False
    if img_dir is None:
        img_dir = os.path.join(os.path.dirname(output_dir), "images")

    # Try to read first image to get actual dimensions and check for alpha
    if os.path.exists(img_dir):
        for fname in sorted(os.listdir(img_dir)):
            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                img_path = os.path.join(img_dir, fname)
                img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
                if img is not None:
                    height, width = img.shape[:2]
                    has_alpha = img.shape[2] == 4 if len(img.shape) == 3 else False
                    print(f"[COLMAP] Using actual image dimensions: {width}x{height}")
                    if has_alpha:
                        print(f"[COLMAP] Alpha channel detected - will use foreground-based point initialization")
                    break

    # Unity default vertical FOV is 60 degrees
    # focal = height / (2 * tan(vfov/2))
    unity_vfov = math.radians(60)
    focal = height / (2 * math.tan(unity_vfov / 2))
    with open(os.path.join(output_dir, "cameras.txt"), "w") as f:
        f.write("# Camera list with one line of data per camera:\n")
        f.write("#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n")
        f.write(f"1 PINHOLE {width} {height} {focal} {focal} {width/2} {height/2}\n")

    # Build map rotation matrix for camera rotation normalization
    R_map, R_map_inv = build_map_rotation_matrix(map_transform)

    # 2. images.txt
    with open(os.path.join(output_dir, "images.txt"), "w") as f:
        f.write("# Image list with two lines of data per image:\n")
        for i, frame in enumerate(frames):
            p = frame['camPos']
            q = frame['camRot']

            # --- Coordinate Conversion with Normalization ---
            # 1. Convert Unity Quaternion to Rotation Matrix (LHS)
            qx, qy, qz, qw = q['x'], q['y'], q['z'], q['w']
            R_unity = quat_to_mat(qx, qy, qz, qw)
            C_unity = np.array([p['x'], p['y'], p['z']])

            # 2. Normalize position (undo map transform)
            C_local = normalize_position(C_unity, map_transform)

            # 3. Normalize rotation (undo map rotation)
            R_local = R_map_inv @ R_unity

            # 4. Convert LHS to RHS (COLMAP: Y-down)
            S = np.diag([1, -1, 1])
            R_rhs = S @ R_local @ S
            C_rhs = S @ C_local

            # 5. Convert Camera-to-World to World-to-Camera
            R_w2c = R_rhs.T
            t_w2c = -R_w2c @ C_rhs

            # 6. Convert Matrix back to Quaternion for COLMAP images.txt
            qw_c, qx_c, qy_c, qz_c = mat_to_quat(R_w2c)

            # Line 1: IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME
            f.write(f"{i+1} {qw_c} {qx_c} {qy_c} {qz_c} {t_w2c[0]} {t_w2c[1]} {t_w2c[2]} 1 {frame['file_path']}\n\n")

    # 3. points3D.txt (Foreground-based point cloud seeding)
    _write_points3d(
        output_dir, frames, img_dir, map_transform,
        has_alpha, width, height, focal, use_midas
    )


def _write_points3d(output_dir, frames, img_dir, map_transform, has_alpha, width, height, focal, use_midas):
    """Write points3D.txt with foreground-based or object-based initialization."""

    with open(os.path.join(output_dir, "points3D.txt"), "w") as f:
        f.write("# 3D point list with one line of data per point:\n")
        pt_idx = 1

        if has_alpha and os.path.exists(img_dir):
            print(f"[COLMAP] Generating foreground-based initial points from alpha masks...")

            # Try to load MiDaS depth estimator for better depth estimation
            depth_estimator = None
            if use_midas:
                try:
                    from src.adapters.depth_estimator import DepthEstimator
                    depth_estimator = DepthEstimator("MiDaS_small")
                    print(f"[COLMAP] Using MiDaS for depth estimation (better point quality)")
                except Exception as e:
                    print(f"[COLMAP] MiDaS not available ({e}), using distance-based depth")
            else:
                print(f"[COLMAP] MiDaS disabled, using distance-based depth")

            all_points = []

            # Sample from more frames for better coverage (10 frames instead of 5)
            sample_frames = frames[::max(1, len(frames)//10)]

            for frame_idx, frame in enumerate(sample_frames):
                points = _backproject_frame(
                    frame, img_dir, map_transform, width, height, focal, depth_estimator
                )
                all_points.extend(points)
                print(f"[COLMAP] Processed frame {frame_idx+1}/{len(sample_frames)}: {frame['file_path']}")

            # Write unique points (remove duplicates by rounding)
            seen = set()
            for pt, r, g, b in all_points:
                key = (round(pt[0], 2), round(pt[1], 2), round(pt[2], 2))
                if key not in seen:
                    seen.add(key)
                    f.write(f"{pt_idx} {pt[0]} {pt[1]} {pt[2]} {r} {g} {b} 0\n")
                    pt_idx += 1

            print(f"[COLMAP] Generated {pt_idx - 1} foreground-based initial points")

        else:
            # Fallback: use object position-based seeding (original behavior)
            print(f"[COLMAP] Using object position-based point initialization (no alpha)")
            for frame in frames:
                obj_pos = frame.get('objPos', {'x': 0, 'y': 0, 'z': 0})
                obj_unity = np.array([obj_pos['x'], obj_pos['y'], obj_pos['z']])

                # Normalize object position (same as camera)
                obj_local = normalize_position(obj_unity, map_transform)

                # Apply same flip as transforms_json (Z flip for NeRF convention)
                ox, oy, oz = obj_local[0], obj_local[1], -obj_local[2]

                # Sample a few points around the object to give 4DGS a dense start
                for dx in [-0.1, 0, 0.1]:
                    for dy in [-0.1, 0, 0.1]:
                        for dz in [-0.1, 0, 0.1]:
                            f.write(f"{pt_idx} {ox+dx} {oy+dy} {oz+dz} 128 128 128 0\n")
                            pt_idx += 1


def _backproject_frame(frame, img_dir, map_transform, width, height, focal, depth_estimator):
    """Back-project foreground pixels from a single frame to 3D points."""
    img_path = os.path.join(img_dir, frame['file_path'])
    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
    if img is None or img.shape[2] != 4:
        return []

    alpha = img[:, :, 3]
    foreground_mask = alpha > 127

    # Get foreground pixel coordinates
    fy, fx = np.where(foreground_mask)
    if len(fx) == 0:
        return []

    # Sample up to 500 foreground pixels per frame
    n_samples = min(500, len(fx))
    indices = np.random.choice(len(fx), n_samples, replace=False)
    sampled_x = fx[indices]
    sampled_y = fy[indices]

    # Get camera pose for this frame
    p = frame['camPos']
    q = frame['camRot']
    qx, qy, qz, qw = q['x'], q['y'], q['z'], q['w']
    R_unity = quat_to_mat(qx, qy, qz, qw)
    C_unity = np.array([p['x'], p['y'], p['z']])

    # Normalize camera position and rotation
    C_local = normalize_position(C_unity, map_transform)
    R_map, R_map_inv = build_map_rotation_matrix(map_transform)
    R_local = R_map_inv @ R_unity

    # Convert to NeRF convention (flip Z)
    flip = np.array([[1, 0, 0], [0, 1, 0], [0, 0, -1]])
    R_nerf = flip @ R_local @ flip.T
    C_nerf = flip @ C_local

    # Get reference depth (distance to object center)
    obj_pos = frame.get('objPos', {'x': 0, 'y': 0, 'z': 0})
    obj_unity = np.array([obj_pos['x'], obj_pos['y'], obj_pos['z']])
    obj_local = normalize_position(obj_unity, map_transform)
    obj_nerf = flip @ obj_local
    ref_depth = np.linalg.norm(obj_nerf - C_nerf)
    if ref_depth < 0.1:
        ref_depth = 2.0  # Fallback depth

    # Get per-pixel depth using MiDaS or fallback to uniform depth
    use_midas_depth = False
    if depth_estimator is not None:
        try:
            # Get relative depth map
            rel_depth_map = depth_estimator.estimate(img, normalize=True)

            # Scale relative depth to metric depth using reference distance
            fg_depths = rel_depth_map[foreground_mask]
            if len(fg_depths) > 0:
                median_rel = np.median(fg_depths)
                if median_rel > 0.01:
                    depth_scale = ref_depth / median_rel
                else:
                    depth_scale = ref_depth
            else:
                depth_scale = ref_depth

            use_midas_depth = True
        except Exception as e:
            print(f"[COLMAP] Depth estimation failed for {frame['file_path']}: {e}")

    # Back-project pixels to 3D
    points = []
    cx, cy = width / 2, height / 2
    for px, py in zip(sampled_x, sampled_y):
        # Normalized image coordinates
        x_norm = (px - cx) / focal
        y_norm = (py - cy) / focal

        # Ray direction in camera space (looking along +Z in NeRF convention)
        ray_dir_cam = np.array([x_norm, y_norm, 1.0])
        ray_dir_cam = ray_dir_cam / np.linalg.norm(ray_dir_cam)

        # Transform to world space
        ray_dir_world = R_nerf @ ray_dir_cam

        # Get depth for this pixel
        if use_midas_depth:
            pixel_depth = rel_depth_map[py, px] * depth_scale
            pixel_depth = pixel_depth * (0.95 + 0.1 * np.random.random())
        else:
            pixel_depth = ref_depth * (0.8 + 0.4 * np.random.random())

        point_3d = C_nerf + ray_dir_world * pixel_depth

        # Get color from image
        b, g, r = img[py, px, :3]
        points.append((point_3d, r, g, b))

    return points
